{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20003 ['<unk>', '<s>', '</s>', ',', '的', '.', '\"', '和', '在', '了', '中国', '是', '对', '发展', '与', '美国', '要', '中', '一', '问题']\n",
      "20003 ['<unk>', '<s>', '</s>', 'the', ',', 'and', 'of', '.', 'to', 'in', '\"', 'a', '-', 'is', 'that', \"'s\", 'for', 'on', 'china', 'with']\n"
     ]
    }
   ],
   "source": [
    "def load_vocab(path):\n",
    "    with open(path, 'r', encoding='utf-8') as fr:\n",
    "        vocab = fr.readlines()\n",
    "        vocab = [w.strip('\\n') for w in vocab]\n",
    "    return vocab  \n",
    "\n",
    "vocab_ch = load_vocab('data/vocab.ch')\n",
    "vocab_en = load_vocab('data/vocab.en')\n",
    "print(len(vocab_ch), vocab_ch[:20])\n",
    "print(len(vocab_en), vocab_en[:20])\n",
    "\n",
    "word2id_ch = {w: i for i, w in enumerate(vocab_ch)}\n",
    "id2word_ch = {i: w for i, w in enumerate(vocab_ch)}\n",
    "word2id_en = {w: i for i, w in enumerate(vocab_en)}\n",
    "id2word_en = {i: w for i, w in enumerate(vocab_en)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen_ch: 62\n",
      "maxlen_en: 62\n",
      "[1, 1613, 3, 593, 121, 435, 3, 0, 3, 53, 86, 1139, 1133, 277, 5389, 43, 6148, 966, 4, 694, 3, 429, 1766, 3, 3200, 0, 3, 14170, 15367, 3, 82, 1551, 244, 47, 6422, 6148, 5, 2]\n",
      "[1, 8, 842, 5575, 3, 244, 8, 11059, 3, 18806, 3, 6045, 0, 87, 0, 3, 345, 0, 3, 9035, 1473, 514, 1373, 607, 48, 3, 188, 514, 2760, 2519, 48, 5, 2]\n",
      "[1, 0, 0, 0, 18, 846, 5477, 2829, 2571, 510, 6148, 966, 563, 4, 2132, 3, 96, 88, 6284, 238, 24, 4, 4109, 569, 490, 490, 3, 24, 22, 41, 6, 1585, 2251, 86, 57, 100, 465, 6148, 42, 9364, 5, 2]\n",
      "[1, 0, 1518, 31, 9740, 3872, 11060, 9741, 1779, 332, 0, 4058, 22, 41, 6, 32, 27, 7, 32, 1185, 1230, 3, 37, 42, 843, 29, 4, 7255, 3, 7255, 11061, 1603, 12, 29, 47, 117, 3, 2761, 42, 92, 1473, 14171, 5, 6, 2]\n",
      "[1, 277, 2868, 93, 3, 115, 750, 3200, 2035, 3, 1641, 750, 508, 31, 4578, 16879, 48, 1014, 4451, 5, 2]\n",
      "[1, 9, 1041, 4, 3, 14251, 944, 3532, 189, 89, 5, 3, 14251, 3532, 189, 89, 92, 1501, 1341, 5, 1064, 17, 11, 1570, 362, 448, 1130, 3814, 947, 107, 3, 1206, 4, 2706, 81, 0, 4, 5, 462, 8, 339, 53, 653, 12, 8, 12, 653, 1130, 289, 3, 28, 6, 46, 1242, 7, 2]\n",
      "[1, 9, 148, 382, 225, 4, 3, 284, 7476, 17, 3, 6489, 4, 9, 3, 12137, 462, 4, 75, 137, 3, 3459, 5, 1020, 2864, 8, 3, 1130, 4540, 7, 2]\n",
      "[1, 35, 523, 1301, 37, 18372, 1514, 6, 18372, 1649, 9, 8120, 886, 818, 176, 11, 1130, 1485, 319, 24, 770, 4, 3163, 428, 8, 496, 8, 60, 846, 8, 377, 11, 3815, 8, 60, 1309, 7, 36, 44, 43, 10, 144, 93, 368, 115, 506, 485, 8, 377, 41, 1130, 4540, 51, 90, 360, 747, 2]\n",
      "[1, 0, 4, 11, 9352, 37, 0, 1514, 6, 0, 1649, 9, 8120, 327, 4, 44, 43, 10, 11, 77, 13, 333, 420, 11, 865, 5, 3, 64, 13, 333, 420, 11, 9353, 7, 144, 3, 9353, 1600, 12138, 60, 1309, 4, 60, 1309, 23, 113, 5027, 2809, 7, 10, 2]\n",
      "[1, 123, 194, 101, 905, 485, 4, 194, 1133, 14, 26, 145, 27, 679, 16, 41, 8, 4227, 3, 2706, 625, 5, 178, 144, 31, 372, 4227, 154, 625, 4, 102, 6, 154, 145, 1952, 3, 2065, 360, 11, 1130, 3815, 1731, 7, 2]\n",
      "(100000, 62) (100000, 62)\n",
      "[    1  1613     3   593   121   435     3     0     3    53    86  1139\n",
      "  1133   277  5389    43  6148   966     4   694     3   429  1766     3\n",
      "  3200     0     3 14170 15367     3    82  1551   244    47  6422  6148\n",
      "     5     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2]\n",
      "[    1     8   842  5575     3   244     8 11059     3 18806     3  6045\n",
      "     0    87     0     3   345     0     3  9035  1473   514  1373   607\n",
      "    48     3   188   514  2760  2519    48     5     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2]\n",
      "[   1    0    0    0   18  846 5477 2829 2571  510 6148  966  563    4\n",
      " 2132    3   96   88 6284  238   24    4 4109  569  490  490    3   24\n",
      "   22   41    6 1585 2251   86   57  100  465 6148   42 9364    5    2\n",
      "    2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "    2    2    2    2    2    2]\n",
      "[    1     0  1518    31  9740  3872 11060  9741  1779   332     0  4058\n",
      "    22    41     6    32    27     7    32  1185  1230     3    37    42\n",
      "   843    29     4  7255     3  7255 11061  1603    12    29    47   117\n",
      "     3  2761    42    92  1473 14171     5     6     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2]\n",
      "[    1   277  2868    93     3   115   750  3200  2035     3  1641   750\n",
      "   508    31  4578 16879    48  1014  4451     5     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2]\n",
      "[    1     9  1041     4     3 14251   944  3532   189    89     5     3\n",
      " 14251  3532   189    89    92  1501  1341     5  1064    17    11  1570\n",
      "   362   448  1130  3814   947   107     3  1206     4  2706    81     0\n",
      "     4     5   462     8   339    53   653    12     8    12   653  1130\n",
      "   289     3    28     6    46  1242     7     2     2     2     2     2\n",
      "     2     2]\n",
      "[    1     9   148   382   225     4     3   284  7476    17     3  6489\n",
      "     4     9     3 12137   462     4    75   137     3  3459     5  1020\n",
      "  2864     8     3  1130  4540     7     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2]\n",
      "[    1    35   523  1301    37 18372  1514     6 18372  1649     9  8120\n",
      "   886   818   176    11  1130  1485   319    24   770     4  3163   428\n",
      "     8   496     8    60   846     8   377    11  3815     8    60  1309\n",
      "     7    36    44    43    10   144    93   368   115   506   485     8\n",
      "   377    41  1130  4540    51    90   360   747     2     2     2     2\n",
      "     2     2]\n",
      "[    1     0     4    11  9352    37     0  1514     6     0  1649     9\n",
      "  8120   327     4    44    43    10    11    77    13   333   420    11\n",
      "   865     5     3    64    13   333   420    11  9353     7   144     3\n",
      "  9353  1600 12138    60  1309     4    60  1309    23   113  5027  2809\n",
      "     7    10     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2]\n",
      "[   1  123  194  101  905  485    4  194 1133   14   26  145   27  679\n",
      "   16   41    8 4227    3 2706  625    5  178  144   31  372 4227  154\n",
      "  625    4  102    6  154  145 1952    3 2065  360   11 1130 3815 1731\n",
      "    7    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "    2    2    2    2    2    2]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path, word2id):\n",
    "    with open(path, 'r', encoding='utf-8') as fr:\n",
    "        lines = fr.readlines()\n",
    "        sentences = [line.strip('\\n').split(' ') for line in lines]\n",
    "        sentences = [[word2id['<s>']] + [word2id[w] for w in sentence] + [word2id['</s>']]\n",
    "                     for sentence in sentences]\n",
    "        lens = [len(sentence) for sentence in sentences]  # 统计每行的长度\n",
    "        maxlen = np.max(lens)  # 单行最大长度\n",
    "        return sentences, lens, maxlen\n",
    "\n",
    "# train: training, no beam search, calculate loss\n",
    "# eval: no training, no beam search, calculate loss\n",
    "# infer: no training, beam search, calculate bleu\n",
    "\n",
    "mode = 'train'\n",
    "\n",
    "train_ch, len_train_ch, maxlen_train_ch = load_data('data/train.ch', word2id_ch)\n",
    "train_en, len_train_en, maxlen_train_en = load_data('data/train.en', word2id_en)\n",
    "dev_ch, len_dev_ch, maxlen_dev_ch = load_data('data/dev.ch', word2id_ch)\n",
    "dev_en, len_dev_en, maxlen_dev_en = load_data('data/dev.en', word2id_en)\n",
    "test_ch, len_test_ch, maxlen_test_ch = load_data('data/test.ch', word2id_ch)\n",
    "test_en, len_test_en, maxlen_test_en = load_data('data/test.en', word2id_en)\n",
    "\n",
    "maxlen_ch = np.max([maxlen_train_ch, maxlen_dev_ch, maxlen_test_ch])  # 训练集、验证集、测试集的单行最大长度\n",
    "maxlen_en = np.max([maxlen_train_en, maxlen_dev_en, maxlen_test_en])  # 训练集、验证集、测试集的单行最大长度\n",
    "\n",
    "print('maxlen_ch:', maxlen_ch)\n",
    "print('maxlen_en:', maxlen_en)\n",
    "\n",
    "for x in train_ch[0:5]:\n",
    "    print(x)\n",
    "for x in train_en[0:5]:\n",
    "    print(x)\n",
    "\n",
    "if mode == 'train':\n",
    "    train_ch = pad_sequences(train_ch, maxlen=maxlen_ch, padding='post', value=word2id_ch['</s>'])  # 在句子结尾填充，填充值为 </s> 对应的 id\n",
    "    train_en = pad_sequences(train_en, maxlen=maxlen_en, padding='post', value=word2id_en['</s>'])\n",
    "    print(train_ch.shape, train_en.shape)\n",
    "    for x in train_ch[0:5]:\n",
    "        print(x)\n",
    "    for x in train_en[0:5]:\n",
    "        print(x)\n",
    "elif mode == 'eval':\n",
    "    dev_ch = pad_sequences(dev_ch, maxlen=maxlen_ch, padding='post', value=word2id_ch['</s>'])\n",
    "    dev_en = pad_sequences(dev_en, maxlen=maxlen_en, padding='post', value=word2id_en['</s>'])\n",
    "    print(dev_ch.shape, dev_en.shape)\n",
    "    for x in dev_ch[0:5]:\n",
    "        print(x)\n",
    "    for x in dev_en[0:5]:\n",
    "        print(x)\n",
    "elif mode == 'infer':\n",
    "    test_ch = pad_sequences(test_ch, maxlen=maxlen_ch, padding='post', value=word2id_ch['</s>'])\n",
    "    test_en = pad_sequences(test_en, maxlen=maxlen_en, padding='post', value=word2id_en['</s>'])\n",
    "    print(test_ch.shape, test_en.shape)\n",
    "    for x in test_ch[0:5]:\n",
    "        print(x)\n",
    "    for x in test_en[0:5]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, maxlen_ch])  # 每一批次有若干条数据，每个的长度为 maxlen_ch\n",
    "X_len = tf.placeholder(tf.int32, [None])\n",
    "Y = tf.placeholder(tf.int32, [None, maxlen_en])\n",
    "Y_len = tf.placeholder(tf.int32, [None])\n",
    "Y_in = Y[:, :-1]  # 不包含最后一个 </s>\n",
    "Y_out = Y[:, 1:]  # 不包含开头的 <s>\n",
    "\n",
    "# 参数的两种初始化方式\n",
    "k_initializer = tf.contrib.layers.xavier_initializer()\n",
    "e_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "\n",
    "embedding_size = 512  # 嵌入层维度（词向量维度）\n",
    "hidden_size = 512  # 隐藏层单元数\n",
    "\n",
    "# 如果是训练模式，batch size 为 128，其它模式 batch size 为 16\n",
    "if mode == 'train':\n",
    "    batch_size = 128\n",
    "else:\n",
    "    batch_size = 16\n",
    "\n",
    "with tf.variable_scope('embedding_X'):\n",
    "    embeddings_X = tf.get_variable('weights_X', shape=[len(word2id_ch), embedding_size], initializer=e_initializer)\n",
    "    embedded_X = tf.nn.embedding_lookup(embeddings_X, X) # batch_size, seq_len, embedding_size\n",
    "    \n",
    "with tf.variable_scope('embedding_Y'):\n",
    "    embeddings_Y = tf.get_variable('weights_Y', shape=[len(word2id_en), embedding_size], initializer=e_initializer)\n",
    "    embedded_Y = tf.nn.embedding_lookup(embeddings_Y, Y_in) # batch_size, seq_len, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "==================================================================================================== \n",
      " (<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/transpose_1:0' shape=(?, 62, 512) dtype=float32>, <tf.Tensor 'encoder/ReverseSequence:0' shape=(?, 62, 512) dtype=float32>)\n",
      "==================================================================================================== \n",
      " Tensor(\"encoder/concat:0\", shape=(?, 62, 1024), dtype=float32)\n",
      "==================================================================================================== \n",
      " ((LSTMStateTuple(c=<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(?, 512) dtype=float32>),), (LSTMStateTuple(c=<tf.Tensor 'encoder/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'encoder/bidirectional_rnn/bw/bw/while/Exit_4:0' shape=(?, 512) dtype=float32>),))\n",
      "====================================================================================================\n",
      "0 LSTMStateTuple(c=<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(?, 512) dtype=float32>)\n",
      "1 LSTMStateTuple(c=<tf.Tensor 'encoder/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'encoder/bidirectional_rnn/bw/bw/while/Exit_4:0' shape=(?, 512) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "def single_cell(mode=mode):\n",
    "    \"\"\"\n",
    "    定义 LSTM 单元\n",
    "    @param mode: 模式（训练模式或者其它模式），如果是其它模式则不设置反向随机失活（dropout）\n",
    "    @return cell: LSTM 单元\n",
    "    \"\"\"\n",
    "    if mode == 'train':\n",
    "        keep_prob = 0.8\n",
    "    else:\n",
    "        keep_prob = 1.0\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "\n",
    "def multi_cells(num_layers):\n",
    "    \"\"\"\n",
    "    定义多层 LSTM\n",
    "    @param num_layers: LSTM 层数\n",
    "    @retrun tf.nn.rnn_cell.MultiRNNCell(cells): 返回多层 LSTM 神经网络\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell = single_cell()\n",
    "        cells.append(cell)\n",
    "    return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "编码器部分\n",
    "双向循环、单层 LSTM\n",
    "\"\"\"\n",
    "with tf.variable_scope('encoder'):\n",
    "    num_layers = 1\n",
    "    fw_cell = multi_cells(num_layers)\n",
    "    bw_cell = multi_cells(num_layers)\n",
    "    bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, embedded_X, dtype=tf.float32, sequence_length=X_len)\n",
    "    # fw: batch_size, seq_len, hidden_size\n",
    "    # bw: batch_size, seq_len, hidden_size\n",
    "    print('=' * 100, '\\n', bi_outputs)\n",
    "    \n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    print('=' * 100, '\\n', encoder_outputs) # batch_size, seq_len, 2 * hidden_size\n",
    "    \n",
    "    # 2 tuple(fw & bw), 2 tuple(c & h), batch_size, hidden_size\n",
    "    # ((c, h), (c, h))\n",
    "    # (([?, 512], [?, 512]), ([?, 512], [?, 512]))\n",
    "    print('=' * 100, '\\n', bi_state)\n",
    "    \n",
    "    encoder_state = []\n",
    "    for i in range(num_layers):\n",
    "        encoder_state.append(bi_state[0][i])  # forward\n",
    "        encoder_state.append(bi_state[1][i])  # backward\n",
    "    encoder_state = tuple(encoder_state) # 2 tuple, 2 tuple(c & h), batch_size, hidden_size\n",
    "    \n",
    "    print('=' * 100)\n",
    "    for i in range(len(encoder_state)):\n",
    "        print(i, encoder_state[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder/transpose:0\", shape=(128, ?, 20003), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "解码部分\n",
    "\"\"\"\n",
    "with tf.variable_scope('decoder'):\n",
    "    beam_width = 10\n",
    "    memory = encoder_outputs\n",
    "                                                                                                                                                                                                                                                                                                          \n",
    "    if mode == 'infer':\n",
    "        memory = tf.contrib.seq2seq.tile_batch(memory, beam_width)\n",
    "        X_len_ = tf.contrib.seq2seq.tile_batch(X_len, beam_width)\n",
    "        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, beam_width)\n",
    "        bs = batch_size * beam_width\n",
    "    else:\n",
    "        bs = batch_size\n",
    "        X_len_ = X_len\n",
    "    \n",
    "    attention = tf.contrib.seq2seq.LuongAttention(hidden_size, memory, X_len_, scale=True) # multiplicative\n",
    "    # attention = tf.contrib.seq2seq.BahdanauAttention(hidden_size, memory, X_len_, normalize=True) # additive\n",
    "    cell = multi_cells(num_layers * 2)\n",
    "    cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention, hidden_size, name='attention')\n",
    "    decoder_initial_state = cell.zero_state(bs, tf.float32).clone(cell_state=encoder_state)\n",
    "    \n",
    "    with tf.variable_scope('projected'):\n",
    "        output_layer = tf.layers.Dense(len(word2id_en), use_bias=False, kernel_initializer=k_initializer)\n",
    "    \n",
    "    if mode == 'infer':\n",
    "        start = tf.fill([batch_size], word2id_en['<s>'])\n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell, embeddings_Y, start, word2id_en['</s>'],\n",
    "                                                       decoder_initial_state, beam_width, output_layer)\n",
    "        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                output_time_major=True,\n",
    "                                                maximum_iterations=2 * tf.reduce_max(X_len))\n",
    "        sample_id = outputs.predicted_ids\n",
    "    else:\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(embedded_Y, [maxlen_en - 1 for b in range(batch_size)])\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, decoder_initial_state, output_layer)\n",
    "        \n",
    "        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                                            output_time_major=True)\n",
    "        logits = outputs.rnn_output\n",
    "        logits = tf.transpose(logits, (1, 0, 2))\n",
    "        print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'infer':\n",
    "    with tf.variable_scope('loss'):\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y_out, logits=logits)\n",
    "        mask = tf.sequence_mask(Y_len, tf.shape(Y_out)[1], tf.float32)\n",
    "        loss = tf.reduce_sum(loss * mask) / batch_size\n",
    "\n",
    "if mode == 'train':\n",
    "    learning_rate = tf.Variable(0.0, trainable=False)\n",
    "    params = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, params), 5.0)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).apply_gradients(zip(grads, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:16<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 lr 1.000 perplexity 1280.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 lr 1.000 perplexity 101.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 lr 1.000 perplexity 44.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 lr 1.000 perplexity 29.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 lr 1.000 perplexity 22.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 lr 1.000 perplexity 18.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 lr 1.000 perplexity 16.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 lr 1.000 perplexity 14.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 lr 1.000 perplexity 13.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:12<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 lr 1.000 perplexity 12.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 lr 1.000 perplexity 11.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 lr 1.000 perplexity 10.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 lr 1.000 perplexity 10.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 lr 1.000 perplexity 9.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 lr 1.000 perplexity 9.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:12<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 lr 0.500 perplexity 7.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 lr 0.500 perplexity 6.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 lr 0.250 perplexity 6.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:13<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 lr 0.250 perplexity 5.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [06:12<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 lr 0.125 perplexity 5.53\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if mode == 'train':\n",
    "    saver = tf.train.Saver()\n",
    "    OUTPUT_DIR = 'model_diy'\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.mkdir(OUTPUT_DIR)\n",
    "        \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(OUTPUT_DIR)\n",
    "        \n",
    "    epochs = 20\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        start_decay = int(epochs * 2 / 3)\n",
    "        if e <= start_decay:\n",
    "            lr = 1.0\n",
    "        else:\n",
    "            decay = 0.5 ** (int(4 * (e - start_decay) / (epochs - start_decay)))\n",
    "            lr = 1.0 * decay\n",
    "        sess.run(tf.assign(learning_rate, lr))\n",
    "        \n",
    "        train_ch, len_train_ch, train_en, len_train_en = shuffle(train_ch, len_train_ch, train_en, len_train_en)\n",
    "        \n",
    "        for i in tqdm(range(train_ch.shape[0] // batch_size)):\n",
    "            X_batch = train_ch[i * batch_size: i * batch_size + batch_size]\n",
    "            X_len_batch = len_train_ch[i * batch_size: i * batch_size + batch_size]\n",
    "            Y_batch = train_en[i * batch_size: i * batch_size + batch_size]\n",
    "            Y_len_batch = len_train_en[i * batch_size: i * batch_size + batch_size]\n",
    "            Y_len_batch = [l - 1 for l in Y_len_batch]\n",
    "\n",
    "            feed_dict = {X: X_batch, Y: Y_batch, X_len: X_len_batch, Y_len: Y_len_batch}\n",
    "            _, ls_ = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            total_loss += ls_ * batch_size\n",
    "            total_count += np.sum(Y_len_batch)\n",
    "\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                writer.add_summary(sess.run(summary, \n",
    "                                            feed_dict=feed_dict), \n",
    "                                            e * train_ch.shape[0] // batch_size + i)\n",
    "                writer.flush()\n",
    "        \n",
    "        print('Epoch %d lr %.3f perplexity %.2f' % (e, lr, np.exp(total_loss / total_count)))\n",
    "        saver.save(sess, os.path.join(OUTPUT_DIR, 'nmt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'eval':\n",
    "    saver = tf.train.Saver()\n",
    "    OUTPUT_DIR = 'model_diy'\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(OUTPUT_DIR))\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    for i in tqdm(range(dev_ch.shape[0] // batch_size)):\n",
    "        X_batch = dev_ch[i * batch_size: i * batch_size + batch_size]\n",
    "        X_len_batch = len_dev_ch[i * batch_size: i * batch_size + batch_size]\n",
    "        Y_batch = dev_en[i * batch_size: i * batch_size + batch_size]\n",
    "        Y_len_batch = len_dev_en[i * batch_size: i * batch_size + batch_size]\n",
    "        Y_len_batch = [l - 1 for l in Y_len_batch]\n",
    "        \n",
    "        feed_dict = {X: X_batch, Y: Y_batch, X_len: X_len_batch, Y_len: Y_len_batch}\n",
    "        ls_ = sess.run(loss, feed_dict=feed_dict)\n",
    "        \n",
    "        total_loss += ls_ * batch_size\n",
    "        total_count += np.sum(Y_len_batch)\n",
    "\n",
    "    print('Dev perplexity %.2f' % np.exp(total_loss / total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if mode == 'infer':\n",
    "    saver = tf.train.Saver()\n",
    "    OUTPUT_DIR = 'model_diy'\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(OUTPUT_DIR))\n",
    "    \n",
    "    def translate(ids):\n",
    "        words = [id2word_en[i] for i in ids]\n",
    "        if words[0] == '<s>':\n",
    "            words = words[1:]\n",
    "        if '</s>' in words:\n",
    "            words = words[:words.index('</s>')]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    fw = open('output_test_diy', 'w')\n",
    "    for i in tqdm(range(test_ch.shape[0] // batch_size)):\n",
    "        X_batch = test_ch[i * batch_size: i * batch_size + batch_size]\n",
    "        X_len_batch = len_test_ch[i * batch_size: i * batch_size + batch_size]\n",
    "        Y_batch = test_en[i * batch_size: i * batch_size + batch_size]\n",
    "        Y_len_batch = len_test_en[i * batch_size: i * batch_size + batch_size]\n",
    "        Y_len_batch = [l - 1 for l in Y_len_batch]\n",
    "        \n",
    "        feed_dict = {X: X_batch, Y: Y_batch, X_len: X_len_batch, Y_len: Y_len_batch}\n",
    "        ids = sess.run(sample_id, feed_dict=feed_dict) # seq_len, batch_size, beam_width\n",
    "        ids = np.transpose(ids, (1, 2, 0)) # batch_size, beam_width, seq_len\n",
    "        ids = ids[:, 0, :] # batch_size, seq_len\n",
    "        \n",
    "        for j in range(ids.shape[0]):\n",
    "            sentence = translate(ids[j])\n",
    "            fw.write(sentence + '\\n')\n",
    "    fw.close()\n",
    "    \n",
    "    from nmt.utils.evaluation_utils import evaluate\n",
    "    \n",
    "    for metric in ['bleu', 'rouge']:\n",
    "        score = evaluate('data/test.en', 'output_test_diy', metric)\n",
    "        print(metric, score / 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
